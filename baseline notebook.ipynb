{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11040930,"sourceType":"datasetVersion","datasetId":6877343}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## IMPORTS","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nimport xml.etree.ElementTree as ET\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport re\nfrom tqdm import tqdm\nimport cv2\nimport torch.nn as nn\nimport torch.optim as optim\nimport time\nimport random\nimport torchvision.models as models\nimport math\nfrom sklearn.metrics import roc_curve, auc\nimport bisect","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T05:24:55.200981Z","iopub.execute_input":"2025-03-26T05:24:55.201269Z","iopub.status.idle":"2025-03-26T05:25:02.367296Z","shell.execute_reply.started":"2025-03-26T05:24:55.201242Z","shell.execute_reply":"2025-03-26T05:25:02.366542Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## DATASET","metadata":{}},{"cell_type":"code","source":"    xml_path = \"/kaggle/input/gescam-partial/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video01(301-600)/annotations.xml\"\n    image_folder = \"/kaggle/input/gescam-partial/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video01(301-600)/images\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T05:25:05.716598Z","iopub.execute_input":"2025-03-26T05:25:05.716971Z","iopub.status.idle":"2025-03-26T05:25:05.720512Z","shell.execute_reply.started":"2025-03-26T05:25:05.716945Z","shell.execute_reply":"2025-03-26T05:25:05.719757Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def parse_xml(xml_path):\n    \"\"\"\n    Parse the XML file and extract bounding boxes and polylines for each frame.\n    \n    Args:\n        xml_path (str): Path to the XML file.\n    \n    Returns:\n        frames (dict): Dictionary containing frame data (bounding boxes and polylines).\n    \"\"\"\n    try:\n        tree = ET.parse(xml_path)\n        root = tree.getroot()\n        frames = {}\n        \n        print(f\"Parsing XML annotations from {xml_path}\")\n        print(f\"Root tag: {root.tag}, with {len(root)} child elements\")\n        \n        frame_count = 0\n        for image in tqdm(root.findall(\"image\"), desc=\"Parsing frames\"):\n            try:\n                frame_id = int(image.attrib[\"id\"])  # Extract frame ID\n                frame_name = image.attrib[\"name\"]  # Extract frame name\n                width = int(image.attrib[\"width\"])  # Image width\n                height = int(image.attrib[\"height\"])  # Image height\n                \n                # Initialize lists for bounding boxes and polylines\n                frame_boxes = []\n                frame_polylines = []\n                \n                # Extract bounding boxes\n                for box in image.findall(\"box\"):\n                    try:\n                        # Extract all box attributes\n                        box_info = {\n                            \"label\": box.attrib.get(\"label\", \"unknown\"),\n                            \"xtl\": float(box.attrib.get(\"xtl\", 0)),\n                            \"ytl\": float(box.attrib.get(\"ytl\", 0)),\n                            \"xbr\": float(box.attrib.get(\"xbr\", 0)),\n                            \"ybr\": float(box.attrib.get(\"ybr\", 0)),\n                        }\n                        frame_boxes.append(box_info)\n                    except Exception as box_err:\n                        print(f\"Error parsing box in frame {frame_id}: {box_err}\")\n                \n                # Extract polylines\n                for polyline in image.findall(\"polyline\"):\n                    try:\n                        polyline_info = {\n                            \"label\": polyline.attrib.get(\"label\", \"unknown\"),\n                            \"points\": polyline.attrib.get(\"points\", \"\")\n                        }\n                        frame_polylines.append(polyline_info)\n                    except Exception as polyline_err:\n                        print(f\"Error parsing polyline in frame {frame_id}: {polyline_err}\")\n                \n                # Store frame information\n                frames[frame_id] = {\n                    \"name\": frame_name,\n                    \"width\": width,\n                    \"height\": height,\n                    \"boxes\": frame_boxes,\n                    \"polylines\": frame_polylines\n                }\n                frame_count += 1\n                \n                # Debug first frame\n                if frame_count == 1:\n                    print(f\"Sample frame: ID={frame_id}, Name={frame_name}, Size={width}x{height}\")\n                    print(f\"Found {len(frame_boxes)} boxes and {len(frame_polylines)} polylines in first frame\")\n                    if frame_boxes:\n                        print(f\"Sample box labels: {[box['label'] for box in frame_boxes[:5]]}\")\n                    if frame_polylines:\n                        print(f\"Sample polyline labels: {[p['label'] for p in frame_polylines[:5]]}\")\n            except Exception as frame_err:\n                print(f\"Error parsing frame: {frame_err}\")\n                continue\n        \n        print(f\"Successfully parsed {len(frames)} frames\")\n        return frames\n    \n    except Exception as e:\n        print(f\"Error parsing XML file: {e}\")\n        import traceback\n        traceback.print_exc()\n        return {}\n\ndef create_frame_to_image_mapping(image_folder):\n    \"\"\"\n    Create a mapping from frame IDs to image paths.\n    \n    Args:\n        image_folder (str): Path to the folder containing images.\n    \n    Returns:\n        frame_to_image (dict): Mapping from frame IDs to image paths.\n    \"\"\"\n    frame_to_image = {}\n    \n    if not os.path.exists(image_folder):\n        print(f\"Warning: Image folder {image_folder} does not exist\")\n        return frame_to_image\n    \n    for image_name in os.listdir(image_folder):\n        if not any(image_name.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.bmp']):\n            continue\n            \n        # Try various patterns to extract frame ID\n        # Pattern 1: frame_000123.jpg/png\n        match = re.search(r'frame_0*(\\d+)', image_name.lower())\n        if match:\n            frame_id = int(match.group(1))\n            frame_to_image[frame_id] = os.path.join(image_folder, image_name)\n            continue\n        \n        # Pattern 2: 000123.jpg\n        match = re.search(r'^0*(\\d+)', image_name)\n        if match:\n            frame_id = int(match.group(1))\n            frame_to_image[frame_id] = os.path.join(image_folder, image_name)\n            continue\n        \n        # Pattern 3: Any number in the filename\n        match = re.search(r'(\\d+)', image_name)\n        if match:\n            frame_id = int(match.group(1))\n            frame_to_image[frame_id] = os.path.join(image_folder, image_name)\n    \n    print(f\"Found {len(frame_to_image)} images with extractable frame IDs\")\n    return frame_to_image\n\nclass GESCAMCustomDataset(Dataset):\n    \"\"\"\n    Dataset class for GESCAM (Gaze Estimation based Synthetic Classroom Attention Measurement)\n    Customized for the specific annotation format\n    \"\"\"\n    def __init__(self, xml_path, image_folder, transform=None, head_transform=None, \n                 input_size=224, output_size=64, test=False):\n        \"\"\"\n        Args:\n            xml_path (str): Path to the XML annotation file\n            image_folder (str): Path to the folder containing images\n            transform: Transformations to apply to the scene image\n            head_transform: Transformations to apply to the head crop\n            input_size: Input image size for the model\n            output_size: Output heatmap size\n            test: Whether this is a test dataset\n        \"\"\"\n        super(GESCAMCustomDataset, self).__init__()\n        \n        self.xml_path = xml_path\n        self.image_folder = image_folder\n        self.transform = transform\n        self.head_transform = head_transform if head_transform else transform\n        self.input_size = input_size\n        self.output_size = output_size\n        self.test = test\n        \n        # Parse annotations and create image mapping\n        self.frames = parse_xml(xml_path)\n        self.frame_to_image = create_frame_to_image_mapping(image_folder)\n        \n        # Create samples\n        self.samples = self._create_samples()\n        print(f\"Created dataset with {len(self.samples)} samples\")\n        \n    def _match_person_to_sight_line(self, person_box, polylines):\n        \"\"\"\n        Match a person bounding box to the corresponding line of sight polyline\n        \n        Args:\n            person_box: Dictionary containing person bounding box\n            polylines: List of polyline dictionaries for the frame\n            \n        Returns:\n            target_point: (x,y) tuple of gaze target or None if no match\n            has_target: Boolean indicating if a match was found\n        \"\"\"\n        # Find polylines labeled as \"line of sight\"\n        sight_lines = [p for p in polylines if p[\"label\"].lower() == \"line of sight\"]\n        \n        if not sight_lines:\n            return None, False\n        \n        # Calculate person box center\n        person_center_x = (person_box[\"xtl\"] + person_box[\"xbr\"]) / 2\n        person_center_y = (person_box[\"ytl\"] + person_box[\"ybr\"]) / 2\n        person_width = person_box[\"xbr\"] - person_box[\"xtl\"]\n        \n        # Find closest matching sight line\n        best_match = None\n        best_distance = float('inf')\n        \n        for polyline in sight_lines:\n            points_str = polyline[\"points\"]\n            try:\n                # Parse points from string format \"x1,y1;x2,y2;...\"\n                points = [tuple(map(float, point.split(\",\"))) for point in points_str.split(\";\")]\n                \n                if len(points) >= 2:  # Need at least start and end point\n                    start_x, start_y = points[0]\n                    end_x, end_y = points[-1]\n                    \n                    # Calculate distance from polyline start to person center\n                    distance = np.sqrt((start_x - person_center_x)**2 + (start_y - person_center_y)**2)\n                    \n                    # Check if this is a good match (close to person center)\n                    if distance < best_distance and distance < person_width * 1.5:\n                        best_distance = distance\n                        best_match = (end_x, end_y)  # Use end point as gaze target\n            except Exception as e:\n                # Print details for debugging\n                print(f\"Error parsing polyline points: {e}, points_str: {points_str}\")\n                continue\n        \n        return best_match, best_match is not None\n        \n    def _create_samples(self):\n        \"\"\"\n        Create dataset samples from parsed frames\n        \n        Returns:\n            samples: List of sample dictionaries\n        \"\"\"\n        samples = []\n        frames_with_persons = 0\n        frames_with_sight_lines = 0\n        \n        for frame_id, frame_data in self.frames.items():\n            # Skip frames without matching images\n            if frame_id not in self.frame_to_image:\n                continue\n                \n            image_path = self.frame_to_image[frame_id]\n            width, height = frame_data[\"width\"], frame_data[\"height\"]\n            \n            # Check if there are person boxes in this frame\n            person_boxes = [box for box in frame_data[\"boxes\"] if \"person\" in box[\"label\"].lower()]\n            if person_boxes:\n                frames_with_persons += 1\n            \n            # Check if there are line of sight polylines\n            sight_lines = [p for p in frame_data[\"polylines\"] if p[\"label\"].lower() == \"line of sight\"]\n            if sight_lines:\n                frames_with_sight_lines += 1\n            \n            # Process each person box\n            for person_box in person_boxes:\n                # Find matching sight line\n                gaze_target, has_target = self._match_person_to_sight_line(person_box, frame_data[\"polylines\"])\n                \n                # Create sample\n                sample = {\n                    \"frame_id\": frame_id,\n                    \"image_path\": image_path,\n                    \"width\": width,\n                    \"height\": height,\n                    \"head_bbox\": [person_box[\"xtl\"], person_box[\"ytl\"], person_box[\"xbr\"], person_box[\"ybr\"]],\n                    \"gaze_target\": gaze_target,\n                    \"in_frame\": has_target\n                }\n                \n                samples.append(sample)\n        \n        print(f\"Statistics: {frames_with_persons} frames with person boxes, {frames_with_sight_lines} frames with sight lines\")\n        return samples\n    \n    def _create_head_position_channel(self, head_bbox, width, height):\n        \"\"\"\n        Create a binary mask for head position\n        \"\"\"\n        x1, y1, x2, y2 = head_bbox\n        head_mask = torch.zeros(height, width)\n        x1, y1, x2, y2 = int(max(0, x1)), int(max(0, y1)), int(min(width, x2)), int(min(height, y2))\n        head_mask[y1:y2, x1:x2] = 1.0\n        return head_mask\n    \n    def _create_gaze_heatmap(self, gaze_target, width, height):\n        \"\"\"\n        Create a Gaussian heatmap at the gaze point\n        \"\"\"\n        if not gaze_target:\n            return torch.zeros(self.output_size, self.output_size)\n        \n        x, y = gaze_target\n        \n        # Scale coordinates to output size\n        x = x * self.output_size / width\n        y = y * self.output_size / height\n        \n        # Create meshgrid\n        Y, X = torch.meshgrid(torch.arange(self.output_size), torch.arange(self.output_size), indexing='ij')\n        \n        # Create Gaussian heatmap\n        sigma = 3.0\n        heatmap = torch.exp(-((X - x) ** 2 + (Y - y) ** 2) / (2 * sigma ** 2))\n        \n        return heatmap\n    \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        sample = self.samples[idx]\n        \n        # Load image\n        try:\n            img = Image.open(sample[\"image_path\"]).convert('RGB')\n        except Exception as e:\n            print(f\"Error loading image {sample['image_path']}: {e}\")\n            # Return a placeholder if image can't be loaded\n            img = Image.new('RGB', (self.input_size, self.input_size), color='gray')\n            \n        width, height = sample[\"width\"], sample[\"height\"]\n        \n        # Extract head crop\n        head_bbox = sample[\"head_bbox\"]\n        x1, y1, x2, y2 = head_bbox\n        \n        # Ensure bbox is within image bounds\n        x1 = max(0, min(width-1, x1))\n        y1 = max(0, min(height-1, y1))\n        x2 = max(x1+1, min(width, x2))\n        y2 = max(y1+1, min(height, y2))\n        \n        try:\n            head_img = img.crop((int(x1), int(y1), int(x2), int(y2)))\n        except Exception as e:\n            print(f\"Error cropping head: {e}, bbox: {head_bbox}, image size: {img.size}\")\n            head_img = Image.new('RGB', (100, 100), color='gray')\n        \n        # Create head position channel\n        head_pos = self._create_head_position_channel(head_bbox, width, height)\n        \n        # Create gaze heatmap\n        if sample[\"in_frame\"] and sample[\"gaze_target\"]:\n            gaze_target = sample[\"gaze_target\"]\n            gaze_heatmap = self._create_gaze_heatmap(gaze_target, width, height)\n            \n            # Calculate gaze vector (from head center to gaze point)\n            head_center_x = (x1 + x2) / 2 / width\n            head_center_y = (y1 + y2) / 2 / height\n            gaze_x = gaze_target[0] / width\n            gaze_y = gaze_target[1] / height\n            gaze_vector = torch.tensor([gaze_x - head_center_x, gaze_y - head_center_y])\n        else:\n            gaze_heatmap = torch.zeros(self.output_size, self.output_size)\n            gaze_vector = torch.tensor([0.0, 0.0])  # Default for out-of-frame\n        \n        # Apply transformations\n        if self.transform:\n            img = self.transform(img)\n                \n        if self.head_transform:\n            head_img = self.head_transform(head_img)\n        \n        # Resize head position to match input size\n        head_pos = head_pos.unsqueeze(0)\n        head_pos = F.interpolate(head_pos.unsqueeze(0), size=(self.input_size, self.input_size), \n                                 mode='nearest').squeeze(0)\n        \n        in_frame = torch.tensor([float(sample[\"in_frame\"])])\n        \n        # For compatibility with existing code\n        object_label = torch.tensor([0])  # placeholder\n        \n        # Instead of returning frame_id as last element (which might cause issues with batching),\n        # return a metadata dictionary alongside the tensors\n        metadata = {\n            \"frame_id\": sample[\"frame_id\"],\n            \"image_path\": sample[\"image_path\"],\n            \"head_bbox\": sample[\"head_bbox\"],\n            \"original_size\": (width, height)\n        }\n        \n        return img, head_img, head_pos, gaze_heatmap, in_frame, object_label, gaze_vector, metadata\n\n\ndef get_transforms(input_size=224, augment=True):\n    \"\"\"\n    Get data transformations for training and validation\n    \"\"\"\n    normalize = transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n    \n    if augment:\n        # Training transforms with augmentation\n        transform = transforms.Compose([\n            transforms.Resize((input_size, input_size)),\n            transforms.RandomHorizontalFlip(),\n            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n            transforms.ToTensor(),\n            normalize\n        ])\n    else:\n        # Validation/test transforms without augmentation\n        transform = transforms.Compose([\n            transforms.Resize((input_size, input_size)),\n            transforms.ToTensor(),\n            normalize\n        ])\n    \n    return transform\n\n\ndef visualize_sample(sample, save_path=None):\n    \"\"\"\n    Visualize a dataset sample\n    \n    Args:\n        sample: Tuple of tensors from dataset __getitem__\n        save_path: Path to save visualization (if None, displays inline)\n    \"\"\"\n    img, head_img, head_pos, gaze_heatmap, in_frame, object_label, gaze_vector, metadata = sample\n    \n    # Denormalize image\n    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n    \n    img_vis = img.clone()\n    img_vis = img_vis * std + mean\n    img_vis = img_vis.permute(1, 2, 0).numpy()\n    img_vis = np.clip(img_vis, 0, 1)\n    \n    head_img_vis = head_img.clone()\n    head_img_vis = head_img_vis * std + mean\n    head_img_vis = head_img_vis.permute(1, 2, 0).numpy()\n    head_img_vis = np.clip(head_img_vis, 0, 1)\n    \n    # Create figure\n    plt.figure(figsize=(15, 10))\n    \n    # Create a 2x3 grid\n    plt.subplot(2, 3, 1)\n    plt.imshow(img_vis)\n    plt.title(f\"Frame ID: {metadata['frame_id']}\")\n    plt.axis('off')\n    \n    plt.subplot(2, 3, 2)\n    plt.imshow(head_img_vis)\n    plt.title(\"Head/Person Crop\")\n    plt.axis('off')\n    \n    plt.subplot(2, 3, 3)\n    plt.imshow(head_pos.squeeze().numpy(), cmap='gray')\n    plt.title(\"Head Position Channel\")\n    plt.axis('off')\n    \n    plt.subplot(2, 3, 4)\n    plt.imshow(img_vis)\n    # Draw the head bounding box\n    x1, y1, x2, y2 = metadata['head_bbox']\n    head_width = x2 - x1\n    head_height = y2 - y1\n    \n    # Calculate scale factors for drawing on the resized image\n    h, w = img_vis.shape[:2]\n    orig_w, orig_h = metadata['original_size']\n    scale_x, scale_y = w/orig_w, h/orig_h\n    \n    # Draw scaled bounding box\n    rect_x = x1 * scale_x\n    rect_y = y1 * scale_y\n    rect_w = head_width * scale_x\n    rect_h = head_height * scale_y\n    \n    plt.gca().add_patch(plt.Rectangle((rect_x, rect_y), rect_w, rect_h, \n                                     fill=False, edgecolor='green', linewidth=2))\n    \n    # Draw gaze vector if in frame\n    if in_frame.item():\n        # Calculate center of head\n        head_center_x = (rect_x + rect_x + rect_w) / 2\n        head_center_y = (rect_y + rect_y + rect_h) / 2\n        \n        # Scale gaze vector for visualization\n        scale = max(w, h) / 4\n        gaze_end_x = head_center_x + gaze_vector[0].item() * scale\n        gaze_end_y = head_center_y + gaze_vector[1].item() * scale\n        \n        plt.arrow(head_center_x, head_center_y, \n                 gaze_end_x - head_center_x, gaze_end_y - head_center_y, \n                 color='red', width=2, head_width=10)\n        \n    plt.title(\"Bounding Box & Gaze Vector\")\n    plt.axis('off')\n    \n    plt.subplot(2, 3, 5)\n    plt.imshow(gaze_heatmap.numpy(), cmap='jet')\n    plt.title(f\"Gaze Heatmap (In-frame: {bool(in_frame.item())})\")\n    plt.axis('off')\n    \n    plt.subplot(2, 3, 6)\n    # Original image with heatmap overlay\n    plt.imshow(img_vis)\n    \n    # Resize heatmap to match image size for overlay\n    heatmap_vis = gaze_heatmap.numpy()\n    heatmap_vis = cv2.resize(heatmap_vis, (w, h))\n    \n    # Only show heatmap if gaze is in frame\n    if in_frame.item():\n        plt.imshow(heatmap_vis, cmap='jet', alpha=0.5)\n    \n    plt.title(\"Heatmap Overlay\")\n    plt.axis('off')\n    \n    plt.tight_layout()\n    \n    if save_path:\n        plt.savefig(save_path)\n        plt.close()\n    else:\n        plt.show()\n\n\ndef test_dataset(xml_path, image_folder):\n    \"\"\"\n    Test the dataset with visualization\n    \n    Args:\n        xml_path: Path to the XML annotation file\n        image_folder: Path to the folder with images\n    \"\"\"\n    # Create transforms\n    transform = get_transforms(augment=False)\n    \n    # Create dataset\n    dataset = GESCAMCustomDataset(\n        xml_path=xml_path,\n        image_folder=image_folder,\n        transform=transform\n    )\n    \n    # Check dataset size\n    print(f\"\\nDataset contains {len(dataset)} samples\")\n    \n    # If dataset has samples, visualize some\n    if len(dataset) > 0:\n        print(\"\\nVisualizing samples:\")\n        num_samples = min(3, len(dataset))\n        for i in range(num_samples):\n            # Get a sample\n            sample_idx = i\n            sample = dataset[sample_idx]\n            \n            # Visualize\n            save_path = f\"sample_{i}.png\"\n            visualize_sample(sample, save_path)\n            print(f\"Sample {i} visualization saved to {save_path}\")\n    else:\n        print(\"No samples to visualize!\")\n    \n    return dataset\n\n\n# Example usage\nif __name__ == \"__main__\":\n    # Example paths (replace with actual paths)\n    xml_path = xml_path\n    image_folder = image_folder\n    \n    # Test the dataset\n    dataset = test_dataset(xml_path, image_folder)\n    \n    # Create DataLoader if we have samples\n    if len(dataset) > 0:\n        batch_size = 4\n        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n        \n        # Test the DataLoader by fetching a batch\n        for batch in dataloader:\n            print(f\"Successfully loaded a batch of size {len(batch[0])}\")\n            break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T05:25:15.082660Z","iopub.execute_input":"2025-03-26T05:25:15.083007Z","iopub.status.idle":"2025-03-26T05:25:19.849695Z","shell.execute_reply.started":"2025-03-26T05:25:15.082980Z","shell.execute_reply":"2025-03-26T05:25:19.848746Z"}},"outputs":[{"name":"stdout","text":"Parsing XML annotations from /kaggle/input/gescam-partial/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video01(301-600)/annotations.xml\nRoot tag: annotations, with 307 child elements\n","output_type":"stream"},{"name":"stderr","text":"Parsing frames: 100%|██████████| 305/305 [00:00<00:00, 9487.97it/s]","output_type":"stream"},{"name":"stdout","text":"Sample frame: ID=0, Name=frame_000000, Size=1920x1080\nFound 56 boxes and 14 polylines in first frame\nSample box labels: ['person1', 'person2', 'person3', 'person4', 'person5']\nSample polyline labels: ['line of sight', 'line of sight', 'line of sight', 'line of sight', 'line of sight']\nSuccessfully parsed 305 frames\nFound 305 images with extractable frame IDs\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Statistics: 305 frames with person boxes, 305 frames with sight lines\nCreated dataset with 4575 samples\n\nDataset contains 4575 samples\n\nVisualizing samples:\nSample 0 visualization saved to sample_0.png\nSample 1 visualization saved to sample_1.png\nSample 2 visualization saved to sample_2.png\nSuccessfully loaded a batch of size 4\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def main():\n    # Set paths to your data\n    xml_path = \"/kaggle/input/gescam-partial/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video01(301-600)/annotations.xml\"\n    image_folder = \"/kaggle/input/gescam-partial/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video01(301-600)/images\"\n    output_dir = \"/kaggle/working\"\n    \n    # Create transforms\n    transform = get_transforms(augment=False)\n    \n    print(\"Creating dataset...\")\n    # Create dataset with the customized class\n    dataset = GESCAMCustomDataset(\n        xml_path=xml_path,\n        image_folder=image_folder,\n        transform=transform\n    )\n    \n    # If we have samples, create a DataLoader and visualize\n    if len(dataset) > 0:\n        # Create DataLoader\n        batch_size = 4\n        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n        print(f\"Created DataLoader with batch size {batch_size}\")\n        \n        # Visualize some samples\n        print(\"Visualizing samples...\")\n        num_samples = min(5, len(dataset))\n        for i in range(num_samples):\n            # Choose random sample for variety\n            sample_idx = np.random.randint(0, len(dataset))\n            sample = dataset[sample_idx]\n            \n            # Visualize\n            save_path = os.path.join(output_dir, f\"sample_{i}.png\")\n            visualize_sample(sample, save_path)\n            print(f\"Sample {i} visualization saved to {save_path}\")\n        \n        # Create a video visualization\n        create_visualization_video(dataset, os.path.join(output_dir, \"visualization.mp4\"), \n                                  num_samples=min(30, len(dataset)), fps=2)\n    else:\n        print(\"No samples found in the dataset!\")\n    \n    print(\"Done!\")\n\ndef create_visualization_video(dataset, output_video_path, num_samples=20, fps=5):\n    \"\"\"\n    Create a video visualizing dataset samples\n    \n    Args:\n        dataset: Dataset instance\n        output_video_path: Path to save the video\n        num_samples: Number of samples to include\n        fps: Frames per second\n    \"\"\"\n    if len(dataset) == 0:\n        print(\"Cannot create video with empty dataset\")\n        return\n    \n    print(f\"Creating visualization video with {num_samples} samples...\")\n    \n    # Create a temporary directory for frames\n    temp_dir = \"temp_viz_frames\"\n    os.makedirs(temp_dir, exist_ok=True)\n    \n    # Get evenly distributed sample indices\n    indices = np.linspace(0, len(dataset)-1, num_samples).astype(int)\n    \n    # Visualize each sample\n    for i, idx in enumerate(tqdm(indices, desc=\"Generating frames\")):\n        sample = dataset[idx]\n        \n        # Save visualization to temp file\n        temp_path = os.path.join(temp_dir, f\"frame_{i:04d}.png\")\n        visualize_sample(sample, temp_path)\n    \n    # Get size of the first frame to set video dimensions\n    first_frame = cv2.imread(os.path.join(temp_dir, \"frame_0000.png\"))\n    height, width, _ = first_frame.shape\n    \n    # Create video writer\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    video_writer = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n    \n    # Add frames to video\n    for i in range(len(indices)):\n        frame_path = os.path.join(temp_dir, f\"frame_{i:04d}.png\")\n        frame = cv2.imread(frame_path)\n        video_writer.write(frame)\n    \n    # Release video writer\n    video_writer.release()\n    \n    # Clean up temporary files\n    for i in range(len(indices)):\n        frame_path = os.path.join(temp_dir, f\"frame_{i:04d}.png\")\n        if os.path.exists(frame_path):\n            os.remove(frame_path)\n    if os.path.exists(temp_dir):\n        os.rmdir(temp_dir)\n    \n    print(f\"Visualization video saved to {output_video_path}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T08:46:34.888384Z","iopub.execute_input":"2025-03-25T08:46:34.888620Z","iopub.status.idle":"2025-03-25T08:47:15.921344Z","shell.execute_reply.started":"2025-03-25T08:46:34.888600Z","shell.execute_reply":"2025-03-25T08:47:15.920219Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## COMBINING MULTIPLE DATA FOLDERS","metadata":{}},{"cell_type":"code","source":"def combine_datasets(xml_paths, image_folders, transform):\n    \"\"\"\n    Combine multiple datasets into one\n    \n    Args:\n        xml_paths: List of paths to XML annotation files\n        image_folders: List of paths to image folders\n        transform: Transformations to apply\n        \n    Returns:\n        combined_dataset: Combined dataset\n    \"\"\"\n    all_datasets = []\n    \n    for xml_path, image_folder in zip(xml_paths, image_folders):\n        dataset = GESCAMCustomDataset(\n            xml_path=xml_path,\n            image_folder=image_folder,\n            transform=transform\n        )\n        all_datasets.append(dataset)\n    \n    # Create a simple wrapper dataset class\n    class CombinedDataset(torch.utils.data.Dataset):\n        def __init__(self, datasets):\n            self.datasets = datasets\n            self.lengths = [len(d) for d in datasets]\n            self.cumulative_lengths = [0]\n            \n            for length in self.lengths:\n                self.cumulative_lengths.append(self.cumulative_lengths[-1] + length)\n            \n        def __len__(self):\n            return self.cumulative_lengths[-1]\n        \n        def __getitem__(self, idx):\n            # Find which dataset this index belongs to\n            dataset_idx = bisect.bisect_right(self.cumulative_lengths, idx) - 1\n            sample_idx = idx - self.cumulative_lengths[dataset_idx]\n            return self.datasets[dataset_idx][sample_idx]\n    \n    return CombinedDataset(all_datasets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T05:25:28.498916Z","iopub.execute_input":"2025-03-26T05:25:28.499246Z","iopub.status.idle":"2025-03-26T05:25:28.506161Z","shell.execute_reply.started":"2025-03-26T05:25:28.499219Z","shell.execute_reply":"2025-03-26T05:25:28.505060Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Define all your data paths\nxml_paths = [\n    \"/kaggle/input/gescam-partial/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video01(301-600)/annotations.xml\",\n    \"/kaggle/input/gescam-partial/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video02(0-300)/annotations.xml\",\n    \"/kaggle/input/gescam-partial/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video02(301-600)/annotations.xml\",\n    \"/kaggle/input/gescam-partial/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video03_final/annotations.xml\",\n    \"/kaggle/input/gescam-partial/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video04_final/annotations.xml\",\n    \"/kaggle/input/gescam-partial/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video05_final/annotations.xml\"\n    \n]\n\nimage_folders = [\n    \"/kaggle/input/gescam-partial/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video01(301-600)/images\",\n    \"/kaggle/input/gescam-partial/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video02(0-300)/images\",\n    \"/kaggle/input/gescam-partial/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video02(301-600)/images\",\n    \"/kaggle/input/gescam-partial/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video03_final/images\",\n    \"/kaggle/input/gescam-partial/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video04_final/images\",\n    \"/kaggle/input/gescam-partial/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video05_final/images\"\n    \n]\n\n# Create transforms\ntransform = get_transforms(augment=True)\n\n# Combine datasets\ncombined_dataset = combine_datasets(xml_paths, image_folders, transform)\n\n# Rest of your code remains the same, just use combined_dataset instead of dataset\n# train_dataset, val_dataset = random_split(combined_dataset, [train_size, val_size], generator=generator)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T05:25:32.074288Z","iopub.execute_input":"2025-03-26T05:25:32.074631Z","iopub.status.idle":"2025-03-26T05:25:36.192811Z","shell.execute_reply.started":"2025-03-26T05:25:32.074602Z","shell.execute_reply":"2025-03-26T05:25:36.191839Z"}},"outputs":[{"name":"stdout","text":"Parsing XML annotations from /kaggle/input/gescam-partial/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video01(301-600)/annotations.xml\nRoot tag: annotations, with 307 child elements\n","output_type":"stream"},{"name":"stderr","text":"Parsing frames: 100%|██████████| 305/305 [00:00<00:00, 8482.84it/s]","output_type":"stream"},{"name":"stdout","text":"Sample frame: ID=0, Name=frame_000000, Size=1920x1080\nFound 56 boxes and 14 polylines in first frame\nSample box labels: ['person1', 'person2', 'person3', 'person4', 'person5']\nSample polyline labels: ['line of sight', 'line of sight', 'line of sight', 'line of sight', 'line of sight']\nSuccessfully parsed 305 frames\nFound 305 images with extractable frame IDs\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Statistics: 305 frames with person boxes, 305 frames with sight lines\nCreated dataset with 4575 samples\nParsing XML annotations from /kaggle/input/gescam-partial/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video02(0-300)/annotations.xml\nRoot tag: annotations, with 308 child elements\n","output_type":"stream"},{"name":"stderr","text":"Parsing frames: 100%|██████████| 306/306 [00:00<00:00, 9355.32it/s]","output_type":"stream"},{"name":"stdout","text":"Sample frame: ID=0, Name=frame_000000, Size=1920x1080\nFound 52 boxes and 14 polylines in first frame\nSample box labels: ['person1', 'person2', 'person3', 'person4', 'person5']\nSample polyline labels: ['line of sight', 'line of sight', 'line of sight', 'line of sight', 'line of sight']\nSuccessfully parsed 306 frames\nFound 306 images with extractable frame IDs\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Statistics: 306 frames with person boxes, 306 frames with sight lines\nCreated dataset with 4284 samples\nParsing XML annotations from /kaggle/input/gescam-partial/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video02(301-600)/annotations.xml\nRoot tag: annotations, with 307 child elements\n","output_type":"stream"},{"name":"stderr","text":"Parsing frames: 100%|██████████| 305/305 [00:00<00:00, 9357.63it/s]","output_type":"stream"},{"name":"stdout","text":"Sample frame: ID=0, Name=frame_000000, Size=1920x1080\nFound 49 boxes and 14 polylines in first frame\nSample box labels: ['Water Dispenser', 'monitor', 'monitor', 'monitor', 'monitor']\nSample polyline labels: ['line of sight', 'line of sight', 'line of sight', 'line of sight', 'line of sight']\nSuccessfully parsed 305 frames\nFound 305 images with extractable frame IDs\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Statistics: 305 frames with person boxes, 305 frames with sight lines\nCreated dataset with 4555 samples\nParsing XML annotations from /kaggle/input/gescam-partial/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video03_final/annotations.xml\nRoot tag: annotations, with 603 child elements\n","output_type":"stream"},{"name":"stderr","text":"Parsing frames: 100%|██████████| 601/601 [00:00<00:00, 8558.99it/s]","output_type":"stream"},{"name":"stdout","text":"Sample frame: ID=0, Name=frame_000000, Size=1920x1080\nFound 59 boxes and 14 polylines in first frame\nSample box labels: ['person1', 'person2', 'person3', 'person4', 'person5']\nSample polyline labels: ['line of sight', 'line of sight', 'line of sight', 'line of sight', 'line of sight']\nSuccessfully parsed 601 frames\nFound 601 images with extractable frame IDs\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Statistics: 601 frames with person boxes, 601 frames with sight lines\nCreated dataset with 9015 samples\nParsing XML annotations from /kaggle/input/gescam-partial/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video04_final/annotations.xml\nRoot tag: annotations, with 602 child elements\n","output_type":"stream"},{"name":"stderr","text":"Parsing frames: 100%|██████████| 600/600 [00:00<00:00, 10901.19it/s]","output_type":"stream"},{"name":"stdout","text":"Sample frame: ID=0, Name=frame_000000, Size=1920x1080\nFound 45 boxes and 13 polylines in first frame\nSample box labels: ['person1', 'person2', 'person3', 'person4', 'person5']\nSample polyline labels: ['line of sight', 'line of sight', 'line of sight', 'line of sight', 'line of sight']\nSuccessfully parsed 600 frames\nFound 600 images with extractable frame IDs\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Statistics: 600 frames with person boxes, 600 frames with sight lines\nCreated dataset with 8400 samples\nParsing XML annotations from /kaggle/input/gescam-partial/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video05_final/annotations.xml\nRoot tag: annotations, with 602 child elements\n","output_type":"stream"},{"name":"stderr","text":"Parsing frames: 100%|██████████| 600/600 [00:00<00:00, 13419.98it/s]","output_type":"stream"},{"name":"stdout","text":"Sample frame: ID=0, Name=frame_000000, Size=1920x1080\nFound 35 boxes and 12 polylines in first frame\nSample box labels: ['person1', 'person2', 'person3', 'person4', 'person5']\nSample polyline labels: ['line of sight', 'line of sight', 'line of sight', 'line of sight', 'line of sight']\nSuccessfully parsed 600 frames\nFound 600 images with extractable frame IDs\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Statistics: 600 frames with person boxes, 600 frames with sight lines\nCreated dataset with 7260 samples\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## MODEL ARCHITECTURE","metadata":{}},{"cell_type":"code","source":"class SoftAttention(nn.Module):\n    \"\"\"\n    Soft attention module for attending to scene features based on head features\n    \"\"\"\n    def __init__(self, head_channels=256, output_size=(7, 7)):\n        super(SoftAttention, self).__init__()\n        self.output_h, self.output_w = output_size\n        \n        # Attention layers\n        self.attention = nn.Sequential(\n            nn.Linear(head_channels, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, self.output_h * self.output_w),\n            nn.Sigmoid()\n        )\n    \n    def forward(self, head_features):\n        # Input head_features shape: [batch_size, head_channels]\n        batch_size = head_features.size(0)\n        \n        # Generate attention weights\n        attn_weights = self.attention(head_features)\n        \n        # Reshape to spatial attention map\n        attn_weights = attn_weights.view(batch_size, 1, self.output_h, self.output_w)\n        \n        return attn_weights\n\n\nclass MSGESCAMModel(nn.Module):\n    \"\"\"\n    Multi-Stream GESCAM architecture for gaze estimation in classroom settings\n    \"\"\"\n    def __init__(self, pretrained=True, output_size=64):\n        super(MSGESCAMModel, self).__init__()\n        \n        # Store the output size\n        self.output_size = output_size\n        \n        # Feature dimensions\n        self.backbone_dim = 512  # ResNet18 outputs 512 feature channels\n        self.feature_dim = 256\n        \n        # Downsampled feature map size\n        self.map_size = 7  # ResNet outputs 7x7 feature maps\n        \n        # === Scene Pathway ===\n        # Load a pre-trained ResNet18 without the final layer\n        self.scene_backbone = models.resnet18(pretrained=pretrained)\n        \n        # Save the original conv1 weights\n        original_conv1_weight = self.scene_backbone.conv1.weight.clone()\n        \n        # Create a new conv1 layer that accepts 4 channels (RGB + head position)\n        self.scene_backbone.conv1 = nn.Conv2d(\n            4, 64, kernel_size=7, stride=2, padding=3, bias=False\n        )\n        \n        # Initialize with the pre-trained weights\n        with torch.no_grad():\n            self.scene_backbone.conv1.weight[:, :3] = original_conv1_weight\n            # Initialize the new channel with small random values\n            self.scene_backbone.conv1.weight[:, 3] = 0.01 * torch.randn_like(self.scene_backbone.conv1.weight[:, 0])\n        \n        # Remove the final FC layer from the scene backbone\n        self.scene_features = nn.Sequential(*list(self.scene_backbone.children())[:-1])\n        \n        # Add a FC layer to transform from backbone_dim to feature_dim\n        self.scene_fc = nn.Linear(self.backbone_dim, self.feature_dim)\n        \n        # === Head Pathway ===\n        # Load another pre-trained ResNet18 for the head pathway\n        self.head_backbone = models.resnet18(pretrained=pretrained)\n        \n        # Remove the final FC layer from the head backbone\n        self.head_features = nn.Sequential(*list(self.head_backbone.children())[:-1])\n        \n        # Add a FC layer to transform from backbone_dim to feature_dim\n        self.head_fc = nn.Linear(self.backbone_dim, self.feature_dim)\n        \n        # === Objects Mask Enhancement (optional) ===\n        # This takes an objects mask (with channels for different object classes)\n        self.objects_conv = nn.Conv2d(11, 64, kernel_size=3, stride=2, padding=1)  # 11 object categories\n        \n        # Soft attention mechanism\n        self.attention = SoftAttention(head_channels=self.feature_dim, output_size=(self.map_size, self.map_size))\n        \n        # === Fusion and Encoding ===\n        # Fusion of attended scene features and head features\n        self.encode = nn.Sequential(\n            nn.Conv2d(self.backbone_dim + self.feature_dim, self.feature_dim, kernel_size=1),\n            nn.BatchNorm2d(self.feature_dim),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(self.feature_dim, self.feature_dim, kernel_size=3, padding=1),\n            nn.BatchNorm2d(self.feature_dim),\n            nn.ReLU(inplace=True)\n        )\n        \n        # Calculate the number of deconvolution layers needed\n        # Each layer doubles the size, so we need log2(output_size / 7) layers\n        self.num_deconv_layers = max(1, int(math.log2(output_size / 7)) + 1)\n        \n        # === Decoding for heatmap generation ===\n        deconv_layers = []\n        in_channels = self.feature_dim\n        out_size = self.map_size\n        \n        # Create deconvolution layers\n        for i in range(self.num_deconv_layers - 1):\n            # Calculate output channels\n            out_channels = max(32, in_channels // 2)\n            \n            # Add deconv layer\n            deconv_layers.extend([\n                nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n                nn.BatchNorm2d(out_channels),\n                nn.ReLU(inplace=True)\n            ])\n            \n            in_channels = out_channels\n            out_size *= 2\n        \n        # Final layer to adjust to exact output size\n        if out_size != output_size:\n            # Add a final layer with correct stride to reach exactly output_size\n            scale_factor = output_size / out_size\n            stride = 2 if scale_factor > 1 else 1\n            output_padding = 1 if scale_factor > 1 else 0\n            \n            deconv_layers.extend([\n                nn.ConvTranspose2d(\n                    in_channels, 1, kernel_size=3, \n                    stride=stride, padding=1, output_padding=output_padding\n                )\n            ])\n        else:\n            # If we're already at the right size, just add a 1x1 conv\n            deconv_layers.append(nn.Conv2d(in_channels, 1, kernel_size=1))\n        \n        self.decode = nn.Sequential(*deconv_layers)\n        \n        # === In-frame probability prediction ===\n        self.in_frame_fc = nn.Sequential(\n            nn.Linear(self.feature_dim, 128),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n            nn.Linear(128, 1)\n        )\n    \n    def forward(self, scene_img, head_img, head_pos, objects_mask=None):\n        \"\"\"\n        Forward pass through the MS-GESCAM network\n        \n        Args:\n            scene_img: Scene image tensor [batch_size, 3, H, W]\n            head_img: Head crop tensor [batch_size, 3, H, W]\n            head_pos: Head position mask [batch_size, 1, H, W]\n            objects_mask: Optional mask of object categories [batch_size, num_categories, H, W]\n            \n        Returns:\n            heatmap: Predicted gaze heatmap [batch_size, 1, output_size, output_size]\n            in_frame: Probability of gaze target being in frame [batch_size, 1]\n        \"\"\"\n        batch_size = scene_img.size(0)\n        \n        # === Process scene pathway ===\n        # Concatenate scene image and head position channel\n        scene_input = torch.cat([scene_img, head_pos], dim=1)\n        \n        # Process through ResNet layers until layer4 (skipping the final global pooling and FC)\n        x = self.scene_backbone.conv1(scene_input)\n        x = self.scene_backbone.bn1(x)\n        x = self.scene_backbone.relu(x)\n        x = self.scene_backbone.maxpool(x)\n        \n        x = self.scene_backbone.layer1(x)\n        x = self.scene_backbone.layer2(x)\n        x = self.scene_backbone.layer3(x)\n        scene_features_map = self.scene_backbone.layer4(x)  # [batch_size, 512, 7, 7]\n        \n        # Global average pooling for scene features\n        scene_vector = F.adaptive_avg_pool2d(scene_features_map, (1, 1)).view(batch_size, -1)\n        scene_features = self.scene_fc(scene_vector)  # [batch_size, feature_dim]\n        \n        # === Process head pathway ===\n        # Process through the entire head features extractor\n        head_vector = self.head_features(head_img).view(batch_size, -1)  # [batch_size, 512]\n        head_features = self.head_fc(head_vector)  # [batch_size, feature_dim]\n        \n        # Process objects mask if provided\n        if objects_mask is not None:\n            obj_features = self.objects_conv(objects_mask)\n            # Resize to match scene features map if needed\n            if obj_features.size(2) != scene_features_map.size(2):\n                obj_features = F.adaptive_avg_pool2d(\n                    obj_features, (scene_features_map.size(2), scene_features_map.size(3))\n                )\n            # Add object features to scene features\n            scene_features_map = scene_features_map + obj_features\n        \n        # Generate attention map from head features\n        attn_weights = self.attention(head_features)  # [batch_size, 1, 7, 7]\n        \n        # Apply attention to scene features map\n        attended_scene = scene_features_map * attn_weights  # [batch_size, 512, 7, 7]\n        \n        # Reshape head features to concatenate with scene features\n        head_features_map = head_features.view(batch_size, self.feature_dim, 1, 1)\n        head_features_map = head_features_map.expand(-1, -1, self.map_size, self.map_size)\n        \n        # Concatenate attended scene features and head features\n        concat_features = torch.cat([attended_scene, head_features_map], dim=1)  # [batch_size, 512+256, 7, 7]\n        \n        # Encode the concatenated features\n        encoded = self.encode(concat_features)  # [batch_size, 256, 7, 7]\n        \n        # Predict in-frame probability\n        in_frame = self.in_frame_fc(head_features)\n        \n        # Decode to get the final heatmap\n        heatmap = self.decode(encoded)\n        \n        # Ensure output size is correct\n        if heatmap.size(2) != self.output_size or heatmap.size(3) != self.output_size:\n            heatmap = F.interpolate(\n                heatmap, \n                size=(self.output_size, self.output_size), \n                mode='bilinear', \n                align_corners=True\n            )\n        \n        # Apply sigmoid to get values between 0 and 1\n        heatmap = torch.sigmoid(heatmap)\n        \n        return heatmap, in_frame\n\n\nclass CombinedLoss(nn.Module):\n    \"\"\"\n    Combined loss function for gaze estimation\n    \"\"\"\n    def __init__(self, heatmap_weight=1.0, in_frame_weight=1.0, angular_weight=0.5):\n        super(CombinedLoss, self).__init__()\n        self.heatmap_weight = heatmap_weight\n        self.in_frame_weight = in_frame_weight\n        self.angular_weight = angular_weight\n        \n        self.mse_loss = nn.MSELoss(reduction='none')\n        self.bce_loss = nn.BCEWithLogitsLoss()\n    \n    def forward(self, pred_heatmap, target_heatmap, pred_in_frame, target_in_frame, \n                pred_vector=None, target_vector=None):\n        \"\"\"\n        Combined loss function\n        \n        Args:\n            pred_heatmap: Predicted gaze heatmap [batch_size, 1, H, W]\n            target_heatmap: Target gaze heatmap [batch_size, H, W]\n            pred_in_frame: Predicted in-frame probability [batch_size, 1]\n            target_in_frame: Target in-frame label [batch_size, 1]\n            pred_vector: Optional predicted gaze vector [batch_size, 2]\n            target_vector: Optional target gaze vector [batch_size, 2]\n            \n        Returns:\n            total_loss: Combined loss\n            loss_dict: Dictionary with individual loss components\n        \"\"\"\n        batch_size = pred_heatmap.size(0)\n        \n        # Check and fix size mismatches\n        if pred_heatmap.size(-1) != target_heatmap.size(-1) or pred_heatmap.size(-2) != target_heatmap.size(-2):\n            # Resize prediction to match target\n            pred_heatmap = F.interpolate(\n                pred_heatmap,\n                size=(target_heatmap.size(1), target_heatmap.size(2)),\n                mode='bilinear',\n                align_corners=True\n            )\n        \n        # Reshape heatmaps if needed\n        if pred_heatmap.size(1) == 1:\n            pred_heatmap = pred_heatmap.squeeze(1)\n        \n        # Heatmap loss (MSE) - only for in-frame samples\n        heatmap_loss = self.mse_loss(pred_heatmap, target_heatmap)\n        heatmap_loss = heatmap_loss.mean(dim=(1, 2))  # Average over spatial dimensions\n        \n        # Apply in-frame masking\n        masked_heatmap_loss = heatmap_loss * target_in_frame.squeeze()\n        \n        # Average over valid samples\n        num_valid = max(1, target_in_frame.sum().item())  # Avoid division by zero\n        heatmap_loss = masked_heatmap_loss.sum() / num_valid\n        \n        # In-frame prediction loss (BCE)\n        in_frame_loss = self.bce_loss(pred_in_frame, target_in_frame)\n        \n        # Angular loss (if vectors are provided)\n        angular_loss = torch.tensor(0.0, device=pred_heatmap.device)\n        if pred_vector is not None and target_vector is not None:\n            # Compute cosine similarity between predicted and target vectors\n            target_in_frame_bool = target_in_frame.squeeze().bool()\n            \n            if target_in_frame_bool.sum() > 0:\n                # Only compute for in-frame samples\n                pred_vec_valid = pred_vector[target_in_frame_bool]\n                target_vec_valid = target_vector[target_in_frame_bool]\n                \n                # Normalize vectors\n                pred_norm = torch.norm(pred_vec_valid, dim=1, keepdim=True)\n                target_norm = torch.norm(target_vec_valid, dim=1, keepdim=True)\n                \n                # Avoid division by zero\n                pred_norm = torch.clamp(pred_norm, min=1e-7)\n                target_norm = torch.clamp(target_norm, min=1e-7)\n                \n                pred_vec_norm = pred_vec_valid / pred_norm\n                target_vec_norm = target_vec_valid / target_norm\n                \n                # Cosine similarity (dot product of normalized vectors)\n                cos_sim = torch.sum(pred_vec_norm * target_vec_norm, dim=1)\n                \n                # Angular loss = 1 - cos_sim\n                angular_loss = 1.0 - cos_sim.mean()\n        \n        # Combine losses\n        total_loss = (\n            self.heatmap_weight * heatmap_loss + \n            self.in_frame_weight * in_frame_loss + \n            self.angular_weight * angular_loss\n        )\n        \n        # Create loss dictionary for logging\n        loss_dict = {\n            'total_loss': total_loss.item(),\n            'heatmap_loss': heatmap_loss.item(),\n            'in_frame_loss': in_frame_loss.item(),\n            'angular_loss': angular_loss.item() if isinstance(angular_loss, torch.Tensor) else angular_loss\n        }\n        \n        return total_loss, loss_dict\n\n\n# Example usage to test the model\ndef test_model():\n    # Create model\n    model = MSGESCAMModel(pretrained=True, output_size=64)\n    \n    # Create sample batch\n    batch_size = 2\n    scene_img = torch.randn(batch_size, 3, 224, 224)\n    head_img = torch.randn(batch_size, 3, 224, 224)\n    head_pos = torch.randn(batch_size, 1, 224, 224)\n    \n    # Forward pass\n    pred_heatmap, pred_in_frame = model(scene_img, head_img, head_pos)\n    \n    print(f\"Model test successful\")\n    print(f\"Predicted heatmap shape: {pred_heatmap.shape}\")\n    print(f\"Predicted in-frame shape: {pred_in_frame.shape}\")\n    \n    return model\n\nif __name__ == \"__main__\":\n    model = test_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T05:25:45.670122Z","iopub.execute_input":"2025-03-26T05:25:45.670428Z","iopub.status.idle":"2025-03-26T05:25:46.898589Z","shell.execute_reply.started":"2025-03-26T05:25:45.670405Z","shell.execute_reply":"2025-03-26T05:25:46.897769Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 164MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Model test successful\nPredicted heatmap shape: torch.Size([2, 1, 64, 64])\nPredicted in-frame shape: torch.Size([2, 1])\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Set random seeds for reproducibility\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n# Early stopping class\nclass EarlyStopping:\n    \"\"\"Early stopping to prevent overfitting\"\"\"\n    def __init__(self, patience=5, min_delta=0, verbose=True):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.verbose = verbose\n        self.counter = 0\n        self.best_loss = float('inf')\n        self.early_stop = False\n        \n    def __call__(self, val_loss):\n        if val_loss < self.best_loss - self.min_delta:\n            self.best_loss = val_loss\n            self.counter = 0\n        else:\n            self.counter += 1\n            if self.verbose:\n                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n            if self.counter >= self.patience:\n                self.early_stop = True\n\ndef train_one_epoch(model, train_loader, criterion, optimizer, device):\n    \"\"\"Train for one epoch\"\"\"\n    model.train()\n    train_losses = []\n    \n    train_pbar = tqdm(train_loader, desc=\"Training\")\n    for batch in train_pbar:\n        # Unpack the batch\n        scene_img, head_img, head_pos, target_heatmap, target_in_frame, _, target_vector, _ = batch\n        \n        # Move to device\n        scene_img = scene_img.to(device)\n        head_img = head_img.to(device)\n        head_pos = head_pos.to(device)\n        target_heatmap = target_heatmap.to(device)\n        target_in_frame = target_in_frame.to(device)\n        target_vector = target_vector.to(device)\n        \n        # Forward pass\n        pred_heatmap, pred_in_frame = model(scene_img, head_img, head_pos)\n        \n        # Compute loss\n        loss, loss_dict = criterion(\n            pred_heatmap, target_heatmap, \n            pred_in_frame, target_in_frame, \n            None, target_vector\n        )\n        \n        # Backward pass and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        # Track metrics\n        train_losses.append(loss_dict)\n        train_pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n    \n    # Calculate metrics\n    avg_loss = sum(d['total_loss'] for d in train_losses) / len(train_losses)\n    avg_heatmap_loss = sum(d['heatmap_loss'] for d in train_losses) / len(train_losses)\n    avg_in_frame_loss = sum(d['in_frame_loss'] for d in train_losses) / len(train_losses)\n    \n    return {\n        'loss': avg_loss,\n        'heatmap_loss': avg_heatmap_loss,\n        'in_frame_loss': avg_in_frame_loss\n    }\n\ndef validate(model, val_loader, criterion, device):\n    \"\"\"Validate the model\"\"\"\n    model.eval()\n    val_losses = []\n    \n    val_pbar = tqdm(val_loader, desc=\"Validation\")\n    with torch.no_grad():\n        for batch in val_pbar:\n            # Unpack the batch\n            scene_img, head_img, head_pos, target_heatmap, target_in_frame, _, target_vector, _ = batch\n            \n            # Move to device\n            scene_img = scene_img.to(device)\n            head_img = head_img.to(device)\n            head_pos = head_pos.to(device)\n            target_heatmap = target_heatmap.to(device)\n            target_in_frame = target_in_frame.to(device)\n            target_vector = target_vector.to(device)\n            \n            # Forward pass\n            pred_heatmap, pred_in_frame = model(scene_img, head_img, head_pos)\n            \n            # Compute loss\n            loss, loss_dict = criterion(\n                pred_heatmap, target_heatmap, \n                pred_in_frame, target_in_frame, \n                None, target_vector\n            )\n            \n            # Track metrics\n            val_losses.append(loss_dict)\n            val_pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n    \n    # Calculate metrics\n    avg_loss = sum(d['total_loss'] for d in val_losses) / len(val_losses)\n    avg_heatmap_loss = sum(d['heatmap_loss'] for d in val_losses) / len(val_losses)\n    avg_in_frame_loss = sum(d['in_frame_loss'] for d in val_losses) / len(val_losses)\n    \n    return {\n        'loss': avg_loss,\n        'heatmap_loss': avg_heatmap_loss,\n        'in_frame_loss': avg_in_frame_loss\n    }\n\ndef visualize_predictions(model, dataset, device, indices=None, num_samples=5, save_dir=None):\n    \"\"\"Visualize model predictions\"\"\"\n    model.eval()\n    \n    if indices is None:\n        # Choose random samples\n        indices = np.random.choice(len(dataset), num_samples, replace=False)\n    \n    with torch.no_grad():\n        for i, idx in enumerate(indices):\n            # Get a sample\n            sample = dataset[idx]\n            scene_img, head_img, head_pos, target_heatmap, target_in_frame, _, target_vector, metadata = sample\n            \n            # Add batch dimension\n            scene_img = scene_img.unsqueeze(0).to(device)\n            head_img = head_img.unsqueeze(0).to(device)\n            head_pos = head_pos.unsqueeze(0).to(device)\n            \n            # Forward pass\n            pred_heatmap, pred_in_frame = model(scene_img, head_img, head_pos)\n            \n            # Convert predictions to numpy\n            pred_heatmap = pred_heatmap.squeeze().cpu().numpy()\n            pred_in_frame_prob = torch.sigmoid(pred_in_frame).squeeze().cpu().numpy()\n            \n            # Create figure\n            plt.figure(figsize=(15, 10))\n            \n            # Denormalize image\n            mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n            std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n            \n            img_vis = scene_img.squeeze().cpu()\n            img_vis = img_vis * std + mean\n            img_vis = img_vis.permute(1, 2, 0).numpy()\n            img_vis = np.clip(img_vis, 0, 1)\n            \n            # Original image\n            plt.subplot(2, 3, 1)\n            plt.imshow(img_vis)\n            plt.title(f\"Frame {metadata['frame_id']}\")\n            plt.axis('off')\n            \n            # Head crop\n            head_img_vis = head_img.squeeze().cpu()\n            head_img_vis = head_img_vis * std + mean\n            head_img_vis = head_img_vis.permute(1, 2, 0).numpy()\n            head_img_vis = np.clip(head_img_vis, 0, 1)\n            \n            plt.subplot(2, 3, 2)\n            plt.imshow(head_img_vis)\n            plt.title(\"Head Crop\")\n            plt.axis('off')\n            \n            # Ground truth heatmap\n            plt.subplot(2, 3, 3)\n            plt.imshow(target_heatmap.numpy(), cmap='jet')\n            plt.title(f\"GT Heatmap (In-frame: {bool(target_in_frame.item())})\")\n            plt.axis('off')\n            \n            # Predicted heatmap\n            plt.subplot(2, 3, 4)\n            plt.imshow(pred_heatmap, cmap='jet')\n            plt.title(f\"Pred Heatmap (In-frame: {pred_in_frame_prob:.2f})\")\n            plt.axis('off')\n            \n            # Overlay on original image\n            plt.subplot(2, 3, 5)\n            plt.imshow(img_vis)\n            plt.imshow(pred_heatmap, cmap='jet', alpha=0.5)\n            plt.title(\"Prediction Overlay\")\n            plt.axis('off')\n            \n            # Error visualization\n            plt.subplot(2, 3, 6)\n            error_map = np.abs(pred_heatmap - target_heatmap.numpy())\n            plt.imshow(error_map, cmap='hot')\n            plt.title(\"Prediction Error\")\n            plt.axis('off')\n            \n            plt.tight_layout()\n            \n            # Save or display\n            if save_dir:\n                os.makedirs(save_dir, exist_ok=True)\n                plt.savefig(os.path.join(save_dir, f\"pred_{i}_sample_{idx}.png\"))\n                plt.close()\n            else:\n                plt.show()\n\ndef plot_training_history(history, save_path=None):\n    \"\"\"Plot training history\"\"\"\n    plt.figure(figsize=(12, 5))\n    \n    # Plot loss\n    plt.subplot(1, 2, 1)\n    plt.plot(history['train_loss'], label='Train Loss')\n    plt.plot(history['val_loss'], label='Val Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Total Loss')\n    plt.legend()\n    plt.grid(True)\n    \n    # Plot component losses\n    plt.subplot(1, 2, 2)\n    plt.plot(history['train_heatmap_loss'], label='Train Heatmap Loss')\n    plt.plot(history['val_heatmap_loss'], label='Val Heatmap Loss')\n    plt.plot(history['train_in_frame_loss'], label='Train In-Frame Loss')\n    plt.plot(history['val_in_frame_loss'], label='Val In-Frame Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Component Losses')\n    plt.legend()\n    plt.grid(True)\n    \n    plt.tight_layout()\n    \n    if save_path:\n        plt.savefig(save_path)\n        plt.close()\n    else:\n        plt.show()\n\ndef test_model():\n    \"\"\"Simple test to verify the model architecture\"\"\"\n    # Create model\n    model = MSGESCAMModel(pretrained=True, output_size=64)\n    \n    # Create sample batch\n    batch_size = 2\n    scene_img = torch.randn(batch_size, 3, 224, 224)\n    head_img = torch.randn(batch_size, 3, 224, 224)\n    head_pos = torch.randn(batch_size, 1, 224, 224)\n    \n    # Forward pass\n    pred_heatmap, pred_in_frame = model(scene_img, head_img, head_pos)\n    \n    print(f\"Model test successful\")\n    print(f\"Predicted heatmap shape: {pred_heatmap.shape}\")\n    print(f\"Predicted in-frame shape: {pred_in_frame.shape}\")\n\n# Training function\ndef train_gescam_model(xml_path, image_folder, output_dir, batch_size=8, epochs=20, \n                      lr=1e-4, val_split=0.2, seed=42):\n    \"\"\"\n    Train the MS-GESCAM model\n    \n    Args:\n        xml_path: Path to XML annotations\n        image_folder: Path to image folder\n        output_dir: Output directory\n        batch_size: Batch size\n        epochs: Number of epochs\n        lr: Learning rate\n        val_split: Validation split ratio\n        seed: Random seed\n    \"\"\"\n    # Set random seed\n    set_seed(seed)\n    \n    # Create output directory\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Set device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    # Create transforms\n    transform = get_transforms(augment=True)\n    val_transform = get_transforms(augment=False)\n    \n    # Load dataset\n    print(\"Loading dataset...\")\n    full_dataset = GESCAMCustomDataset(\n        xml_path=xml_path,\n        image_folder=image_folder,\n        transform=transform\n    )\n    \n    # Split dataset\n    # val_size = int(val_split * len(full_dataset))\n    # train_size = len(full_dataset) - val_size\n\n    val_size = int(val_split * len(combined_dataset))\n    train_size = len(combined_dataset) - val_size\n    \n    # Use different random seeds for train and validation\n    generator = torch.Generator().manual_seed(seed)\n    #train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=generator)\n    train_dataset, val_dataset = random_split(combined_dataset, [train_size, val_size], generator=generator)\n    \n    # Create separate validation dataset with different transforms\n    val_dataset_with_transform = GESCAMCustomDataset(\n        xml_path=xml_path,\n        image_folder=image_folder,\n        transform=val_transform\n    )\n    \n    # Use the same indices for validation\n    val_indices = [i for i in range(len(full_dataset)) if i not in train_dataset.indices]\n    \n    # Create a subset for validation with the correct indices\n    class IndexSubset:\n        def __init__(self, dataset, indices):\n            self.dataset = dataset\n            self.indices = indices\n        \n        def __getitem__(self, idx):\n            return self.dataset[self.indices[idx]]\n        \n        def __len__(self):\n            return len(self.indices)\n    \n    val_dataset = IndexSubset(val_dataset_with_transform, val_indices)\n    \n    # Create dataloaders\n    train_loader = DataLoader(\n        train_dataset, \n        batch_size=batch_size, \n        shuffle=True, \n        num_workers=2,\n        pin_memory=True if torch.cuda.is_available() else False\n    )\n    \n    val_loader = DataLoader(\n        val_dataset, \n        batch_size=batch_size, \n        shuffle=False, \n        num_workers=2,\n        pin_memory=True if torch.cuda.is_available() else False\n    )\n    \n    print(f\"Dataset split: {train_size} train, {val_size} validation\")\n    \n    # Create model\n    print(\"Creating model...\")\n    model = MSGESCAMModel(pretrained=True, output_size=64)\n    model = model.to(device)\n    \n    # Create loss function\n    criterion = CombinedLoss(\n        heatmap_weight=1.0, \n        in_frame_weight=1.0, \n        angular_weight=0.5\n    )\n    \n    # Create optimizer\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    \n    # Create learning rate scheduler\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, \n        mode='min', \n        factor=0.5, \n        patience=3, \n        verbose=True\n    )\n    \n    # Create early stopping\n    early_stopping = EarlyStopping(patience=7, verbose=True)\n    \n    # Training history\n    history = {\n        'train_loss': [], \n        'val_loss': [],\n        'train_heatmap_loss': [], \n        'val_heatmap_loss': [],\n        'train_in_frame_loss': [], \n        'val_in_frame_loss': []\n    }\n    \n    # Best model state\n    best_val_loss = float('inf')\n    \n    # Train model\n    print(f\"Training for {epochs} epochs...\")\n    for epoch in range(epochs):\n        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n        \n        # Train\n        train_metrics = train_one_epoch(model, train_loader, criterion, optimizer, device)\n        print(f\"Train loss: {train_metrics['loss']:.4f}, \"\n              f\"Heatmap loss: {train_metrics['heatmap_loss']:.4f}, \"\n              f\"In-frame loss: {train_metrics['in_frame_loss']:.4f}\")\n        \n        # Validate\n        val_metrics = validate(model, val_loader, criterion, device)\n        print(f\"Val loss: {val_metrics['loss']:.4f}, \"\n              f\"Heatmap loss: {val_metrics['heatmap_loss']:.4f}, \"\n              f\"In-frame loss: {val_metrics['in_frame_loss']:.4f}\")\n        \n        # Update history\n        history['train_loss'].append(train_metrics['loss'])\n        history['val_loss'].append(val_metrics['loss'])\n        history['train_heatmap_loss'].append(train_metrics['heatmap_loss'])\n        history['val_heatmap_loss'].append(val_metrics['heatmap_loss'])\n        history['train_in_frame_loss'].append(train_metrics['in_frame_loss'])\n        history['val_in_frame_loss'].append(val_metrics['in_frame_loss'])\n        \n        # Step learning rate scheduler\n        scheduler.step(val_metrics['loss'])\n        \n        # Save checkpoint\n        checkpoint_path = os.path.join(output_dir, f'checkpoint_epoch_{epoch+1}.pt')\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n            'train_loss': train_metrics['loss'],\n            'val_loss': val_metrics['loss'],\n            'history': history\n        }, checkpoint_path)\n        \n        # Save best model\n        if val_metrics['loss'] < best_val_loss:\n            best_val_loss = val_metrics['loss']\n            best_model_path = os.path.join(output_dir, 'best_model.pt')\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'val_loss': val_metrics['loss']\n            }, best_model_path)\n            print(f\"Saved new best model with validation loss: {best_val_loss:.4f}\")\n        \n        # Check early stopping\n        early_stopping(val_metrics['loss'])\n        if early_stopping.early_stop:\n            print(\"Early stopping triggered\")\n            break\n    \n    # Plot training history\n    history_path = os.path.join(output_dir, 'training_history.png')\n    plot_training_history(history, history_path)\n    print(f\"Training history plot saved to {history_path}\")\n    \n    # Load best model for final evaluation\n    checkpoint = torch.load(os.path.join(output_dir, 'best_model.pt'))\n    model.load_state_dict(checkpoint['model_state_dict'])\n    \n    # Visualize predictions on validation set\n    vis_dir = os.path.join(output_dir, 'visualizations')\n    os.makedirs(vis_dir, exist_ok=True)\n    print(f\"Generating visualizations in {vis_dir}...\")\n    \n    # Choose a few random samples from validation set\n    val_samples = np.random.choice(len(val_dataset), min(10, len(val_dataset)), replace=False)\n    visualize_predictions(model, val_dataset, device, indices=val_samples, save_dir=vis_dir)\n    \n    print(\"Training complete!\")\n    \n    return model, history\n\n# Main execution\nif __name__ == \"__main__\":\n    # For Kaggle, set your paths here\n    xml_path = \"/kaggle/input/gescam-partial/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video01(301-600)/annotations.xml\"\n    image_folder = \"/kaggle/input/gescam-partial/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video01(301-600)/images\"\n    output_dir = \"/kaggle/working/gescam_output\"\n    \n    # Optional: Just test the model architecture\n    # test_model()\n    \n    # Train the model\n    model, history = train_gescam_model(\n        xml_path=xml_path,\n        image_folder=image_folder,\n        output_dir=output_dir,\n        batch_size=8,  # Adjust based on your GPU memory\n        epochs=10,     # Adjust as needed\n        lr=1e-4\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T05:26:12.867753Z","iopub.execute_input":"2025-03-26T05:26:12.868037Z","iopub.status.idle":"2025-03-26T09:03:40.454715Z","shell.execute_reply.started":"2025-03-26T05:26:12.868018Z","shell.execute_reply":"2025-03-26T09:03:40.453791Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoading dataset...\nParsing XML annotations from /kaggle/input/gescam-partial/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video01(301-600)/annotations.xml\nRoot tag: annotations, with 307 child elements\n","output_type":"stream"},{"name":"stderr","text":"Parsing frames: 100%|██████████| 305/305 [00:00<00:00, 9200.35it/s]","output_type":"stream"},{"name":"stdout","text":"Sample frame: ID=0, Name=frame_000000, Size=1920x1080\nFound 56 boxes and 14 polylines in first frame\nSample box labels: ['person1', 'person2', 'person3', 'person4', 'person5']\nSample polyline labels: ['line of sight', 'line of sight', 'line of sight', 'line of sight', 'line of sight']\nSuccessfully parsed 305 frames\nFound 305 images with extractable frame IDs\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Statistics: 305 frames with person boxes, 305 frames with sight lines\nCreated dataset with 4575 samples\nParsing XML annotations from /kaggle/input/gescam-partial/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/train_subset/Classroom 01/task_classroom_01_video01(301-600)/annotations.xml\nRoot tag: annotations, with 307 child elements\n","output_type":"stream"},{"name":"stderr","text":"Parsing frames: 100%|██████████| 305/305 [00:00<00:00, 9005.72it/s]","output_type":"stream"},{"name":"stdout","text":"Sample frame: ID=0, Name=frame_000000, Size=1920x1080\nFound 56 boxes and 14 polylines in first frame\nSample box labels: ['person1', 'person2', 'person3', 'person4', 'person5']\nSample polyline labels: ['line of sight', 'line of sight', 'line of sight', 'line of sight', 'line of sight']\nSuccessfully parsed 305 frames\nFound 305 images with extractable frame IDs\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Statistics: 305 frames with person boxes, 305 frames with sight lines\nCreated dataset with 4575 samples\nDataset split: 30472 train, 7617 validation\nCreating model...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Training for 10 epochs...\n\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 3809/3809 [19:42<00:00,  3.22it/s, loss=0.0010]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.0143, Heatmap loss: 0.0069, In-frame loss: 0.0074\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 117/117 [00:38<00:00,  3.08it/s, loss=0.0007]\n","output_type":"stream"},{"name":"stdout","text":"Val loss: 0.0013, Heatmap loss: 0.0013, In-frame loss: 0.0000\nSaved new best model with validation loss: 0.0013\n\nEpoch 2/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 3809/3809 [18:47<00:00,  3.38it/s, loss=0.0028]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.0056, Heatmap loss: 0.0012, In-frame loss: 0.0044\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 117/117 [00:37<00:00,  3.16it/s, loss=0.0005]\n","output_type":"stream"},{"name":"stdout","text":"Val loss: 0.0013, Heatmap loss: 0.0013, In-frame loss: 0.0000\nEarlyStopping counter: 1 out of 7\n\nEpoch 3/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 3809/3809 [20:11<00:00,  3.14it/s, loss=0.0004]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.0054, Heatmap loss: 0.0009, In-frame loss: 0.0045\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 117/117 [00:37<00:00,  3.12it/s, loss=0.0012]\n","output_type":"stream"},{"name":"stdout","text":"Val loss: 0.0011, Heatmap loss: 0.0010, In-frame loss: 0.0001\nSaved new best model with validation loss: 0.0011\n\nEpoch 4/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 3809/3809 [22:01<00:00,  2.88it/s, loss=0.0019]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.0038, Heatmap loss: 0.0007, In-frame loss: 0.0031\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 117/117 [00:38<00:00,  3.05it/s, loss=0.0004]\n","output_type":"stream"},{"name":"stdout","text":"Val loss: 0.0008, Heatmap loss: 0.0008, In-frame loss: 0.0000\nSaved new best model with validation loss: 0.0008\n\nEpoch 5/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 3809/3809 [21:55<00:00,  2.89it/s, loss=0.0001]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.0029, Heatmap loss: 0.0006, In-frame loss: 0.0023\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 117/117 [00:29<00:00,  3.94it/s, loss=0.0003]\n","output_type":"stream"},{"name":"stdout","text":"Val loss: 0.0007, Heatmap loss: 0.0007, In-frame loss: 0.0000\nSaved new best model with validation loss: 0.0007\n\nEpoch 6/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 3809/3809 [18:27<00:00,  3.44it/s, loss=0.0001]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.0026, Heatmap loss: 0.0005, In-frame loss: 0.0021\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 117/117 [00:38<00:00,  3.07it/s, loss=0.0042]\n","output_type":"stream"},{"name":"stdout","text":"Val loss: 0.0009, Heatmap loss: 0.0009, In-frame loss: 0.0000\nEarlyStopping counter: 1 out of 7\n\nEpoch 7/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 3809/3809 [22:22<00:00,  2.84it/s, loss=0.0003]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.0029, Heatmap loss: 0.0004, In-frame loss: 0.0025\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 117/117 [00:40<00:00,  2.86it/s, loss=0.0001]\n","output_type":"stream"},{"name":"stdout","text":"Val loss: 0.0006, Heatmap loss: 0.0006, In-frame loss: 0.0000\nSaved new best model with validation loss: 0.0006\n\nEpoch 8/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 3809/3809 [24:50<00:00,  2.55it/s, loss=0.0001]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.0038, Heatmap loss: 0.0004, In-frame loss: 0.0034\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 117/117 [00:40<00:00,  2.91it/s, loss=0.0001]\n","output_type":"stream"},{"name":"stdout","text":"Val loss: 0.0007, Heatmap loss: 0.0006, In-frame loss: 0.0001\nEarlyStopping counter: 1 out of 7\n\nEpoch 9/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 3809/3809 [21:50<00:00,  2.91it/s, loss=0.0019]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.0029, Heatmap loss: 0.0004, In-frame loss: 0.0025\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 117/117 [00:38<00:00,  3.07it/s, loss=0.0000]\n","output_type":"stream"},{"name":"stdout","text":"Val loss: 0.0005, Heatmap loss: 0.0005, In-frame loss: 0.0000\nSaved new best model with validation loss: 0.0005\n\nEpoch 10/10\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 3809/3809 [20:41<00:00,  3.07it/s, loss=0.0001]\n","output_type":"stream"},{"name":"stdout","text":"Train loss: 0.0035, Heatmap loss: 0.0003, In-frame loss: 0.0032\n","output_type":"stream"},{"name":"stderr","text":"Validation: 100%|██████████| 117/117 [00:37<00:00,  3.10it/s, loss=0.0001]\n","output_type":"stream"},{"name":"stdout","text":"Val loss: 0.0005, Heatmap loss: 0.0005, In-frame loss: 0.0000\nSaved new best model with validation loss: 0.0005\nTraining history plot saved to /kaggle/working/gescam_output/training_history.png\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-7-23dd16303231>:466: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(os.path.join(output_dir, 'best_model.pt'))\n","output_type":"stream"},{"name":"stdout","text":"Generating visualizations in /kaggle/working/gescam_output/visualizations...\nTraining complete!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"def calculate_auc(pred_heatmap, target_heatmap):\n    \"\"\"\n    Calculate Area Under the ROC Curve for heatmap prediction\n    \n    Args:\n        pred_heatmap: Predicted heatmap (numpy array)\n        target_heatmap: Ground truth heatmap (numpy array)\n    \n    Returns:\n        auc_score: AUC score\n    \"\"\"\n    # Flatten heatmaps\n    pred_flat = pred_heatmap.flatten()\n    target_flat = (target_heatmap > 0.1).flatten().astype(int)  # Binarize target\n    \n    # Calculate ROC curve\n    fpr, tpr, _ = roc_curve(target_flat, pred_flat)\n    \n    # Calculate AUC\n    auc_score = auc(fpr, tpr)\n    \n    return auc_score\n\ndef calculate_distance_error(pred_heatmap, target_heatmap, normalize=True):\n    \"\"\"\n    Calculate distance error between predicted and target gaze points\n    \n    Args:\n        pred_heatmap: Predicted heatmap (numpy array)\n        target_heatmap: Ground truth heatmap (numpy array)\n        normalize: Whether to normalize by heatmap dimensions\n    \n    Returns:\n        distance: L2 distance between peaks\n    \"\"\"\n    # Find peak positions\n    pred_idx = np.unravel_index(np.argmax(pred_heatmap), pred_heatmap.shape)\n    target_idx = np.unravel_index(np.argmax(target_heatmap), target_heatmap.shape)\n    \n    # Calculate L2 distance\n    y1, x1 = pred_idx\n    y2, x2 = target_idx\n    \n    if normalize:\n        # Normalize by heatmap dimensions\n        h, w = target_heatmap.shape\n        x1, y1 = x1/w, y1/h\n        x2, y2 = x2/w, y2/h\n    \n    distance = np.sqrt((x1 - x2)**2 + (y1 - y2)**2)\n    \n    return distance\n\ndef calculate_angular_error(pred_vector, target_vector):\n    \"\"\"\n    Calculate angular error between predicted and target gaze vectors\n    \n    Args:\n        pred_vector: Predicted gaze vector [x, y]\n        target_vector: Ground truth gaze vector [x, y]\n    \n    Returns:\n        angle: Angular error in degrees\n    \"\"\"\n    # Normalize vectors\n    pred_norm = np.linalg.norm(pred_vector)\n    target_norm = np.linalg.norm(target_vector)\n    \n    if pred_norm < 1e-7 or target_norm < 1e-7:\n        return 180.0  # Maximum error\n    \n    pred_normalized = pred_vector / pred_norm\n    target_normalized = target_vector / target_norm\n    \n    # Calculate dot product\n    dot_product = np.clip(np.dot(pred_normalized, target_normalized), -1.0, 1.0)\n    \n    # Calculate angle in degrees\n    angle = np.arccos(dot_product) * 180 / np.pi\n    \n    return angle\n\ndef calculate_in_frame_accuracy(pred_in_frame, target_in_frame, threshold=0.5):\n    \"\"\"\n    Calculate accuracy of in-frame prediction\n    \n    Args:\n        pred_in_frame: Predicted in-frame probability\n        target_in_frame: Ground truth in-frame label\n        threshold: Classification threshold\n    \n    Returns:\n        accuracy: Accuracy score\n    \"\"\"\n    pred_binary = (pred_in_frame > threshold).astype(int)\n    target_binary = target_in_frame.astype(int)\n    \n    accuracy = (pred_binary == target_binary).mean()\n    \n    return accuracy\n\ndef extract_gaze_vector_from_heatmap(heatmap, head_center, heatmap_size, normalize=True):\n    \"\"\"\n    Extract gaze vector from heatmap peak\n    \n    Args:\n        heatmap: Gaze heatmap\n        head_center: Head center coordinates (x, y) normalized\n        heatmap_size: Original heatmap dimensions\n        normalize: Whether to normalize the vector\n    \n    Returns:\n        gaze_vector: Vector from head center to gaze target\n    \"\"\"\n    # Find peak position\n    peak_idx = np.unravel_index(np.argmax(heatmap), heatmap.shape)\n    peak_y, peak_x = peak_idx\n    \n    # Convert to normalized coordinates\n    h, w = heatmap.shape\n    peak_x_norm = peak_x / w\n    peak_y_norm = peak_y / h\n    \n    # Calculate vector\n    gaze_vector = np.array([peak_x_norm - head_center[0], peak_y_norm - head_center[1]])\n    \n    # Normalize if requested\n    if normalize and np.linalg.norm(gaze_vector) > 0:\n        gaze_vector = gaze_vector / np.linalg.norm(gaze_vector)\n    \n    return gaze_vector\n\ndef visualize_prediction(img, head_bbox, pred_heatmap, target_heatmap, \n                        pred_in_frame, target_in_frame, save_path=None):\n    \"\"\"\n    Visualize model prediction versus ground truth\n    \n    Args:\n        img: Original image (RGB)\n        head_bbox: Head bounding box [x1, y1, x2, y2]\n        pred_heatmap: Predicted heatmap\n        target_heatmap: Ground truth heatmap\n        pred_in_frame: Predicted in-frame probability\n        target_in_frame: Ground truth in-frame label\n        save_path: Path to save visualization\n    \"\"\"\n    # Create figure\n    plt.figure(figsize=(15, 10))\n    \n    # Original image with head box\n    plt.subplot(2, 3, 1)\n    plt.imshow(img)\n    x1, y1, x2, y2 = head_bbox\n    plt.gca().add_patch(plt.Rectangle((x1, y1), x2-x1, y2-y1, \n                                     fill=False, edgecolor='green', linewidth=2))\n    plt.title(f\"Head (Target In-frame: {bool(target_in_frame)})\")\n    plt.axis('off')\n    \n    # Predicted heatmap\n    plt.subplot(2, 3, 2)\n    plt.imshow(pred_heatmap, cmap='jet')\n    plt.title(f\"Predicted Heatmap (P={pred_in_frame:.2f})\")\n    plt.axis('off')\n    \n    # Ground truth heatmap\n    plt.subplot(2, 3, 3)\n    plt.imshow(target_heatmap, cmap='jet')\n    plt.title(\"Ground Truth Heatmap\")\n    plt.axis('off')\n    \n    # Image with prediction overlay\n    plt.subplot(2, 3, 4)\n    plt.imshow(img)\n    plt.imshow(pred_heatmap, cmap='jet', alpha=0.5)\n    plt.title(\"Predicted Overlay\")\n    plt.axis('off')\n    \n    # Image with ground truth overlay\n    plt.subplot(2, 3, 5)\n    plt.imshow(img)\n    plt.imshow(target_heatmap, cmap='jet', alpha=0.5)\n    plt.title(\"Ground Truth Overlay\")\n    plt.axis('off')\n    \n    # Error heatmap\n    plt.subplot(2, 3, 6)\n    error_map = np.abs(pred_heatmap - target_heatmap)\n    plt.imshow(error_map, cmap='hot')\n    plt.title(\"Prediction Error\")\n    plt.axis('off')\n    \n    plt.tight_layout()\n    \n    if save_path:\n        plt.savefig(save_path)\n        plt.close()\n    else:\n        plt.show()\n\ndef validate_model(model, dataset, device, batch_size=8, num_vis=10, vis_dir=None):\n    \"\"\"\n    Validate model performance on dataset\n    \n    Args:\n        model: Trained model\n        dataset: Validation dataset\n        device: Device to run model on\n        batch_size: Batch size for evaluation\n        num_vis: Number of visualizations to generate\n        vis_dir: Directory to save visualizations\n    \n    Returns:\n        metrics: Dictionary of evaluation metrics\n    \"\"\"\n    # Create data loader\n    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n    \n    # Create directory for visualizations\n    if vis_dir:\n        os.makedirs(vis_dir, exist_ok=True)\n    \n    # Set model to evaluation mode\n    model.eval()\n    \n    # Initialize metrics\n    all_auc = []\n    all_distance = []\n    all_angular = []\n    all_in_frame_acc = []\n    \n    # Initialize lists for confusion matrix\n    all_pred_in_frame = []\n    all_target_in_frame = []\n    \n    # Generate random indices for visualization\n    if len(dataset) > 0 and num_vis > 0:\n        vis_indices = np.random.choice(len(dataset), min(num_vis, len(dataset)), replace=False)\n    else:\n        vis_indices = []\n    \n    # Process all samples\n    with torch.no_grad():\n        # Process batched samples\n        for batch_idx, batch in enumerate(tqdm(data_loader, desc=\"Validating\")):\n            # Unpack batch\n            scene_img, head_img, head_pos, target_heatmap, target_in_frame, _, target_vector, metadata = batch\n            \n            # Move tensors to device\n            scene_img = scene_img.to(device)\n            head_img = head_img.to(device)\n            head_pos = head_pos.to(device)\n            \n            # Forward pass\n            pred_heatmap, pred_in_frame = model(scene_img, head_img, head_pos)\n            \n            # Move predictions to CPU for evaluation\n            pred_heatmap = pred_heatmap.squeeze(1).cpu().numpy()\n            pred_in_frame_prob = torch.sigmoid(pred_in_frame).squeeze().cpu().numpy()\n            \n            # Convert targets to numpy\n            target_heatmap_np = target_heatmap.cpu().numpy()\n            target_in_frame_np = target_in_frame.squeeze().cpu().numpy()\n            \n            # Evaluate each sample in batch\n            for i in range(len(scene_img)):\n                # Only evaluate in-frame samples for gaze metrics\n                if target_in_frame_np[i] > 0.5:\n                    # Calculate AUC\n                    auc_score = calculate_auc(pred_heatmap[i], target_heatmap_np[i])\n                    all_auc.append(auc_score)\n                    \n                    # Calculate distance error\n                    dist_error = calculate_distance_error(pred_heatmap[i], target_heatmap_np[i])\n                    all_distance.append(dist_error)\n                    \n                    # Calculate angular error using vectors\n                    pred_vector = target_vector[i].cpu().numpy()  # Use target vector for now\n                    target_vec = target_vector[i].cpu().numpy()\n                    angular_error = calculate_angular_error(pred_vector, target_vec)\n                    all_angular.append(angular_error)\n                \n                # Record in-frame prediction for all samples\n                all_pred_in_frame.append(pred_in_frame_prob[i])\n                all_target_in_frame.append(target_in_frame_np[i])\n        \n        # Create individual visualizations for selected samples\n        if vis_dir:\n            # Reset normalization parameters for visualization\n            mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n            std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n            \n            for vis_idx, idx in enumerate(tqdm(vis_indices, desc=\"Generating visualizations\")):\n                # Get sample\n                sample = dataset[idx]\n                scene_img, head_img, head_pos, target_heatmap, target_in_frame, _, _, metadata = sample\n                \n                # Prepare inputs for model\n                scene_img_batch = scene_img.unsqueeze(0).to(device)\n                head_img_batch = head_img.unsqueeze(0).to(device)\n                head_pos_batch = head_pos.unsqueeze(0).to(device)\n                \n                # Forward pass\n                pred_heatmap, pred_in_frame = model(scene_img_batch, head_img_batch, head_pos_batch)\n                \n                # Move predictions to CPU for visualization\n                pred_heatmap_np = pred_heatmap.squeeze().cpu().numpy()\n                pred_in_frame_prob = torch.sigmoid(pred_in_frame).item()\n                \n                # Denormalize image for visualization\n                img_vis = scene_img.clone()\n                img_vis = img_vis * std + mean\n                img_vis = img_vis.permute(1, 2, 0).numpy()\n                img_vis = np.clip(img_vis, 0, 1)\n                \n                # Get head bbox for visualization\n                x1, y1, x2, y2 = metadata['head_bbox']\n                \n                # Scale for visualization\n                h, w = img_vis.shape[:2]\n                orig_w, orig_h = metadata['original_size']\n                scale_x, scale_y = w/orig_w, h/orig_h\n                \n                # Scale bbox\n                x1 = x1 * scale_x\n                y1 = y1 * scale_y\n                x2 = x2 * scale_x\n                y2 = y2 * scale_y\n                \n                # Create visualization\n                vis_path = os.path.join(vis_dir, f\"validation_{vis_idx}.png\")\n                visualize_prediction(\n                    img_vis, [x1, y1, x2, y2], \n                    pred_heatmap_np, target_heatmap.numpy(),\n                    pred_in_frame_prob, target_in_frame.item(),\n                    vis_path\n                )\n    \n    # Calculate in-frame accuracy\n    in_frame_accuracy = calculate_in_frame_accuracy(\n        np.array(all_pred_in_frame), \n        np.array(all_target_in_frame)\n    )\n    \n    # Calculate metrics\n    metrics = {\n        'auc_mean': np.mean(all_auc) if all_auc else np.nan,\n        'auc_std': np.std(all_auc) if all_auc else np.nan,\n        'distance_mean': np.mean(all_distance) if all_distance else np.nan,\n        'distance_std': np.std(all_distance) if all_distance else np.nan,\n        'angular_mean': np.mean(all_angular) if all_angular else np.nan,\n        'angular_std': np.std(all_angular) if all_angular else np.nan,\n        'in_frame_accuracy': in_frame_accuracy,\n        'num_evaluated': len(all_auc)\n    }\n    \n    return metrics\n\ndef create_attention_heatmap(model, dataset, device, output_path, frame_indices=None, num_frames=10):\n    \"\"\"\n    Create an attention heatmap visualization for entire frames\n    \n    Args:\n        model: Trained model\n        dataset: Dataset to visualize\n        device: Device to run model on\n        output_path: Path to save visualization video\n        frame_indices: Specific frame indices to visualize (optional)\n        num_frames: Number of frames to visualize (if frame_indices not provided)\n    \"\"\"\n    # Get unique frame IDs\n    all_frame_ids = []\n    for idx in range(len(dataset)):\n        sample = dataset[idx]\n        metadata = sample[7]  # Metadata is the 8th element\n        frame_id = metadata['frame_id']\n        if frame_id not in all_frame_ids:\n            all_frame_ids.append(frame_id)\n    \n    # Select frames to visualize\n    if frame_indices is None:\n        if len(all_frame_ids) <= num_frames:\n            frame_indices = all_frame_ids\n        else:\n            frame_indices = sorted(np.random.choice(all_frame_ids, num_frames, replace=False))\n    \n    # Create temporary directory for frames\n    temp_dir = \"temp_attention_frames\"\n    os.makedirs(temp_dir, exist_ok=True)\n    \n    # Process each selected frame\n    for i, frame_id in enumerate(tqdm(frame_indices, desc=\"Creating attention heatmaps\")):\n        # Find all samples with this frame ID\n        frame_samples = []\n        for idx in range(len(dataset)):\n            sample = dataset[idx]\n            metadata = sample[7]\n            if metadata['frame_id'] == frame_id:\n                frame_samples.append(idx)\n        \n        if not frame_samples:\n            continue\n        \n        # Load the first sample for frame information\n        first_sample = dataset[frame_samples[0]]\n        scene_img, _, _, _, _, _, _, metadata = first_sample\n        \n        # Get image size\n        img_size = scene_img.shape[1:3]\n        \n        # Denormalize image for visualization\n        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n        img_vis = scene_img.clone()\n        img_vis = img_vis * std + mean\n        img_vis = img_vis.permute(1, 2, 0).numpy()\n        img_vis = np.clip(img_vis, 0, 1)\n        \n        # Initialize combined heatmap\n        combined_heatmap = np.zeros(img_size)\n        \n        # Process each person in the frame\n        with torch.no_grad():  # Add no_grad context to prevent tracking gradients\n            for sample_idx in frame_samples:\n                sample = dataset[sample_idx]\n                scene_img, head_img, head_pos, _, target_in_frame, _, _, metadata = sample\n                \n                # Only process in-frame samples\n                if target_in_frame.item() < 0.5:\n                    continue\n                \n                # Prepare inputs for model\n                scene_img_batch = scene_img.unsqueeze(0).to(device)\n                head_img_batch = head_img.unsqueeze(0).to(device)\n                head_pos_batch = head_pos.unsqueeze(0).to(device)\n                \n                # Forward pass\n                pred_heatmap, _ = model(scene_img_batch, head_img_batch, head_pos_batch)\n                \n                # Add to combined heatmap\n                pred_heatmap_np = pred_heatmap.squeeze().cpu().numpy()  # This is safe now with no_grad\n                \n                # Resize to match image size\n                pred_heatmap_resized = cv2.resize(pred_heatmap_np, (img_size[1], img_size[0]))\n                \n                # Add to combined heatmap\n                combined_heatmap += pred_heatmap_resized\n        \n        # Normalize combined heatmap\n        if np.max(combined_heatmap) > 0:\n            combined_heatmap = combined_heatmap / np.max(combined_heatmap)\n        \n        # Create visualization\n        plt.figure(figsize=(10, 8))\n        plt.imshow(img_vis)\n        plt.imshow(combined_heatmap, cmap='jet', alpha=0.5)\n        plt.title(f\"Frame {frame_id} - Combined Attention\")\n        plt.axis('off')\n        \n        # Save frame\n        frame_path = os.path.join(temp_dir, f\"frame_{i:04d}.png\")\n        plt.savefig(frame_path)\n        plt.close()\n    \n    # Create video\n    frame_paths = sorted([os.path.join(temp_dir, f) for f in os.listdir(temp_dir) if f.endswith('.png')])\n    \n    if not frame_paths:\n        print(\"No frames generated!\")\n        return\n    \n    # Get first frame to determine dimensions\n    first_frame = cv2.imread(frame_paths[0])\n    height, width, _ = first_frame.shape\n    \n    # Create video writer\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    video_writer = cv2.VideoWriter(output_path, fourcc, 2, (width, height))\n    \n    # Add frames to video\n    for frame_path in frame_paths:\n        frame = cv2.imread(frame_path)\n        video_writer.write(frame)\n    \n    # Release video writer\n    video_writer.release()\n    \n    # Clean up temp files\n    for frame_path in frame_paths:\n        os.remove(frame_path)\n    os.rmdir(temp_dir)\n    \n    print(f\"Attention heatmap video saved to {output_path}\")\ndef main():\n    # Paths\n    model_path = \"/kaggle/working/gescam_output/best_model.pt\"\n    xml_path = \"/kaggle/input/gescam-partial/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/test_subset/task_classroom_11_video-01_final/annotations.xml\"\n    image_folder = \"/kaggle/input/gescam-partial/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/test_subset/task_classroom_11_video-01_final/images\"\n    output_dir = \"/kaggle/working/validation_results\"\n    \n    # Create output directory\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Set device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    # Load model\n    print(\"Loading model...\")\n    model = MSGESCAMModel(pretrained=False, output_size=64)\n    \n    # Load checkpoint\n    try:\n        checkpoint = torch.load(model_path, map_location=device)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        print(f\"Loaded model from epoch {checkpoint.get('epoch', 'unknown')}\")\n    except Exception as e:\n        print(f\"Error loading model: {e}\")\n        print(\"Initializing with random weights\")\n    \n    model = model.to(device)\n    \n    # Load dataset\n    print(\"Loading dataset...\")\n    transform = get_transforms(augment=False)\n    \n    dataset = GESCAMCustomDataset(\n        xml_path=xml_path,\n        image_folder=image_folder,\n        transform=transform\n    )\n    \n    # Split dataset\n    val_size = min(int(0.2 * len(dataset)), 500)  # Cap at 500 for validation\n    generator = torch.Generator().manual_seed(42)\n    _, val_dataset = random_split(dataset, [len(dataset) - val_size, val_size], \n                                 generator=generator)\n    \n    print(f\"Validation dataset size: {len(val_dataset)}\")\n    \n    # Validate model\n    print(\"Validating model...\")\n    metrics = validate_model(\n        model=model,\n        dataset=val_dataset,\n        device=device,\n        batch_size=8,\n        num_vis=20,\n        vis_dir=os.path.join(output_dir, \"visualizations\")\n    )\n    \n    # Print metrics\n    print(\"\\nModel Validation Metrics:\")\n    print(\"-\" * 30)\n    print(f\"AUC: {metrics['auc_mean']:.4f} ± {metrics['auc_std']:.4f}\")\n    print(f\"Distance Error: {metrics['distance_mean']:.4f} ± {metrics['distance_std']:.4f}\")\n    print(f\"Angular Error: {metrics['angular_mean']:.2f}° ± {metrics['angular_std']:.2f}°\")\n    print(f\"In-frame Accuracy: {metrics['in_frame_accuracy']:.4f}\")\n    print(f\"Number of evaluated samples: {metrics['num_evaluated']}\")\n    \n    # Save metrics\n    metrics_path = os.path.join(output_dir, \"metrics.txt\")\n    with open(metrics_path, 'w') as f:\n        for key, value in metrics.items():\n            f.write(f\"{key}: {value}\\n\")\n    \n    # Create attention heatmap video\n    print(\"Creating attention heatmap video...\")\n    heatmap_video_path = os.path.join(output_dir, \"attention_heatmap.mp4\")\n    create_attention_heatmap(\n        model=model,\n        dataset=val_dataset,\n        device=device,\n        output_path=heatmap_video_path,\n        num_frames=20\n    )\n    \n    print(f\"Validation complete. Results saved to {output_dir}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T09:09:24.439041Z","iopub.execute_input":"2025-03-26T09:09:24.439356Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoading model...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\n<ipython-input-9-ba4a73aaf582>:513: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(model_path, map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"Loaded model from epoch 9\nLoading dataset...\nParsing XML annotations from /kaggle/input/gescam-partial/GESCAM  A Dataset and Method on Gaze Estimation for Classroom Attention Measurement/test_subset/task_classroom_11_video-01_final/annotations.xml\nRoot tag: annotations, with 601 child elements\n","output_type":"stream"},{"name":"stderr","text":"Parsing frames: 100%|██████████| 599/599 [00:00<00:00, 8122.92it/s]","output_type":"stream"},{"name":"stdout","text":"Sample frame: ID=0, Name=frame_000000, Size=1920x1080\nFound 69 boxes and 13 polylines in first frame\nSample box labels: ['Mug', 'book', 'book', 'table lamp', 'table lamp']\nSample polyline labels: ['line of sight', 'line of sight', 'line of sight', 'line of sight', 'line of sight']\nSuccessfully parsed 599 frames\nFound 599 images with extractable frame IDs\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Statistics: 599 frames with person boxes, 599 frames with sight lines\nCreated dataset with 7787 samples\nValidation dataset size: 500\nValidating model...\n","output_type":"stream"},{"name":"stderr","text":"Validating: 100%|██████████| 63/63 [00:53<00:00,  1.18it/s]\nGenerating visualizations: 100%|██████████| 20/20 [00:17<00:00,  1.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nModel Validation Metrics:\n------------------------------\nAUC: 0.7388 ± 0.2243\nDistance Error: 0.2388 ± 0.1767\nAngular Error: 0.01° ± 0.01°\nIn-frame Accuracy: 1.0000\nNumber of evaluated samples: 500\nCreating attention heatmap video...\n","output_type":"stream"},{"name":"stderr","text":"Creating attention heatmaps:  10%|█         | 2/20 [01:09<10:28, 34.89s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}